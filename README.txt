咳咳...
这个是从某福利网站上下载本子的爬虫。
在运行后会自动生成KeyWord.txt文件。里面存储了需要爬取的关键词。
如果你有新的想要爬取的关键字，请假如在这里面。
如你所见，目前的KeyWrod里只有两个关键字（并且限定了语种），这是因为为了避免给网站运营团队造成麻烦，限制了下载速度（2 images/s）。
你可以在MySpiders文件里看到被注释掉的characters关键字和keyword关键字。
如果你需要，你可以自行在KeyWord里按照yaml语法继续新增你想要的关键字。
更改chineseOnly选项为True/False来开启/关闭仅中文选项。
（第一次完整运行时使用了多达七个关键字，最终运行了22个小时，下载了多达13.5GB的数据，消耗了约65GB流量。所以除非你对爬虫流程进行了优化，否则不要这样做。）

下载结束后会在当前目录下生成Result文件夹，Result文件里按关键字对所有画集进行了分类。
它没有进行去重处理。所以在设置关键字时最好限定语言（比如chinese）。

当前目录下会生成log文件夹，里面存放了按时间命名的下载日志。（并没有什么用，因为如果出错了你并不能通过它来查找出错原因。目前是的。）

它使用的环境如下：
Python 3.6.3
BS4 4.6.4
Scrapy 1.5.1

祝你愉快。
注意身体。